# chp.01-05 TEST (2023.07.07)

> 9:00 - 9:30 테스트 후 ♥영지♥에게 제출



### Can we use decision trees for multi-class prediction problems?

- answer
네




### What is the maximum depth parameter of decision tree training?

- 최대 깊이 수, 의사결정나무에서 트리 깊이를 설정할 때 쓰이는 파라미터이다. 부모노드는 level=0 이후 자식노드 level=1, 2,....





### How can we select a good value for maximum depth parameters?

- 그리드서치, 랜덤서치를 활용하여 최적의 예측성능을 보이는 파라미터를 찾아낸다





### What is pruning in the decision tree?

- 가지치기; 의사결정나무는 과대적합이 발생할 수 있다는 단점이 있는데, 이 단점을 보완하기 위해서 가지치기 기법을 사용한다. max_depth를 조정하여 가지치기를 수행한다







### Cross Validation은 무엇이고 어떻게 해야하나요?

- 교차검증. 주어진 데이터셋에서 훈련데이터셋과 테스트데이터셋으로 나눈 후 훈련데이터를 다시 검증데이터와 훈련데이터로 나누는 것이다. 보통 KFOLD검증 기법을 사용하는데, 첫번째 검증에서 사용된 데이터 일부가 두번째 검증에서는 훈련데이터로 사용되며, 이 과정이 k번 반복된다. 교차검증을 시행하게 되면 실제 테스트케이스에서 확인 하기 전에 검증데이터에서 성능을 확인해봄으로써 과적합을 방지할 수 있다. 



### K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)

- 비지도? 아직 안배우지 않았나용??
초기값(군집 갯수)을 사용자가 설정해주어야하기 때문에 초기값 설정이 잘못되면 낮은 성능을 보일 수 있다





### 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?

- 큰 의사결정 나무는 가지가 많아지기 때문에 해석이 어려워지고 예측 성능이 떨어질 수 있다. 따라서 가지치기를 통한 작은 의사결정 나무모형으로 해석하는 것이 더 효과적일 수 있다









### 앙상블 방법엔 어떤 것들이 있나요?

- 랜덤포레스트, XGBoost, LightGBM, 그래디언트 부스팅, 히스토그램기반 그래디언트











-----

[복습 & 질문 리스트]

- 불순도 간 차이 : 측정방법, 쓰임에는 차이가 없다
- 지니불순도 : 일반적으로 0.5 이하
